{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57791c47-5082-43f4-9919-5ddea2f142e0",
   "metadata": {},
   "source": [
    "# samples\n",
    "This notebook learns directly from audio sample data, in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ba8a56-1b05-4ba5-a220-b32024e77a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26812a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53083622-8025-4e3f-b570-87cacac72cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x, batches=10) -> torch.Tensor:\n",
    "        trim = x.numel() % self.block_size\n",
    "        blocks = (x.numel() - trim) // self.block_size\n",
    "        block_trim = blocks % batches\n",
    "        return x[:-(trim+block_trim*self.block_size)].reshape(-1, batches, self.block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2c0c4b6-8662-4785-8190-d2b1c361b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, device=\"cpu\"):\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    audio = audio.to(device)\n",
    "    if audio.ndim == 2:\n",
    "        return audio.sum(axis=0), sr\n",
    "    else:\n",
    "        return audio, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06069de5-b529-4fcb-abfc-6553456fa274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728102"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = load(\"../data/grains.wav\", \"cuda\")\n",
    "audio.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "901f727b-e3af-42ab-9146-938949e691f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=64\n",
    "BATCH_SIZE=40\n",
    "featurizer = Featurizer(BLOCK_SIZE).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3be978ab-f539-423f-9529-5bdfd03d8135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728102"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60c29395-2486-4096-aa0e-68d4e8ff970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([284, 40, 64])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized = featurizer(audio, BATCH_SIZE)\n",
    "featurized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b4a2b93-f2c2-46c5-b929-0fc16cf0ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, block_size, num_layers, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(block_size, hidden_size, num_layers)\n",
    "        self.output = nn.Linear(hidden_size, block_size)\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        return self.output(output), hidden\n",
    "    def init_hidden(self, batch_size=1, device=\"cpu\"):\n",
    "        shape = (self.lstm.num_layers, batch_size, self.lstm.hidden_size)\n",
    "        return torch.zeros(shape, dtype=torch.float32, device=device), torch.zeros(shape, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc5de790-cf42-4449-a03b-6a69e9dc12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(BLOCK_SIZE, 8, 128).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "20e97181-e9de-4826-be17-b6c064da5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "799a17e7-6187-48d4-b88e-bdcb9d2829bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32130e3b-6cbc-406b-b870-45040cffb4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, loss: 0.010262319818139076\n",
      "Epoch 10000, loss: 0.010068058036267757\n",
      "Epoch 15000, loss: 0.00972263514995575\n",
      "Epoch 20000, loss: 0.009172793477773666\n",
      "Epoch 25000, loss: 0.008535410277545452\n",
      "Epoch 30000, loss: 0.00782046653330326\n",
      "Epoch 35000, loss: 0.007376702036708593\n",
      "Epoch 40000, loss: 0.0069656274281442165\n",
      "Epoch 45000, loss: 0.00668838107958436\n",
      "Epoch 50000, loss: 0.006453242618590593\n",
      "Epoch 55000, loss: 0.00618392089381814\n",
      "Epoch 60000, loss: 0.0059726485051214695\n",
      "Epoch 65000, loss: 0.005835735704749823\n",
      "Epoch 70000, loss: 0.005791875999420881\n",
      "Epoch 75000, loss: 0.005632910877466202\n",
      "Epoch 80000, loss: 0.0055466145277023315\n",
      "Epoch 85000, loss: 0.005518725607544184\n",
      "Epoch 90000, loss: 0.00542448740452528\n",
      "Epoch 95000, loss: 0.005484931170940399\n",
      "Epoch 100000, loss: 0.0053152828477323055\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100000\n",
    "for epoch in range(num_epochs):\n",
    "    init_hidden = model.init_hidden(BATCH_SIZE, \"cuda\")\n",
    "    output, _ = model(featurized, init_hidden)\n",
    "    epoch_loss = loss(output, featurized)\n",
    "    epoch_loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 5000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27084a35-e615-4592-b56a-6d7d1849cd94",
   "metadata": {},
   "source": [
    "## Generation\n",
    "Given an input prompt, generate a continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e741e8d3-2da8-437d-a6eb-f66f73e97c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11376, 1, 64])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = featurizer(audio, 1)\n",
    "prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cf0dff59-34cd-45d0-a8fb-07c3e7a71f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11376, 1, 64])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, hidden = model(prompt, model.init_hidden(1, \"cuda\"))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "772027bd-7a74-4e59-acf4-188078e8f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks=512\n",
    "hidden = model.init_hidden(1, \"cuda\")\n",
    "output, hidden = model(prompt, hidden)\n",
    "new_audio = torch.cat((prompt, output[-1:]))\n",
    "for _ in range(num_blocks):\n",
    "    output, hidden = model(output[-1:], hidden)\n",
    "    new_audio = torch.cat((new_audio, output[-1:]))\n",
    "torchaudio.save(\"../data/output.wav\", new_audio.reshape(-1).unsqueeze(0).to(\"cpu\"), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1ca91942-e5c7-486a-90c3-e948dfe6bc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 736320])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_audio.reshape(-1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd71ead-f042-41c4-823b-0422e817607f",
   "metadata": {},
   "source": [
    "## A more sophisticated approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9b9745f8-946c-4eb1-9a39-6f7f42197284",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_audio, sr = load(\"../data/687984__girlwithsoundrecorder__stefan-a-frog-from-poland.wav\", \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ba33e7bc-673d-4029-bf25-63075adcc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=128\n",
    "BATCH_SIZE=500\n",
    "featurizer = Featurizer(BLOCK_SIZE).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7c576590-a417-4df1-8891-cd5e06f2cf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53, 500, 128])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = featurizer(source_audio, BATCH_SIZE)\n",
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bcfb9401-3e32-486c-9197-f07960bf9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(BLOCK_SIZE, 16, 128).to(\"cuda\")\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37703f-4b72-4391-952d-80cf20f46b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10000\n",
    "for epoch in range(num_epochs):\n",
    "    init_hidden = model.init_hidden(BATCH_SIZE, \"cuda\")\n",
    "    output, _ = model(training, init_hidden)\n",
    "    epoch_loss = loss(output, training)\n",
    "    epoch_loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}, loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68974df-4db3-4a04-9c6b-15f28cc6aa11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
